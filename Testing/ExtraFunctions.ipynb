{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Snippet 1\n",
    "Description: Code used to move a random number of images from one directory to another.\n",
    " \n",
    "Note: This was used to select the 385 images from the LAION-400M dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved: 65.jpg\n",
      "Moved: 2.jpg\n",
      "Moved: 45.jpg\n",
      "Moved: 12.jpg\n",
      "Moved: 123.jpg\n",
      "Moved: 170.jpg\n",
      "Moved: 115.jpg\n",
      "Moved: 179.jpg\n",
      "Moved: 178.jpg\n",
      "Moved: 132.jpg\n",
      "Moved: 81.jpg\n",
      "Moved: 192.jpg\n",
      "Moved: 172.jpg\n",
      "Moved: 84.jpg\n",
      "Moved: 117.jpg\n",
      "Moved: 129.jpg\n",
      "Moved: 79.jpg\n",
      "Moved: 18.jpg\n",
      "Moved: 1.jpg\n",
      "Moved: 21.jpg\n",
      "Moved: 96.jpg\n",
      "Moved: 32.jpg\n",
      "Moved: 38.jpg\n",
      "Moved: 93.jpg\n",
      "Moved: 55.jpg\n",
      "Moved: 35.jpg\n",
      "Moved: 92.jpg\n",
      "Moved: 155.jpg\n",
      "Moved: 27.jpg\n",
      "Moved: 191.jpg\n",
      "Moved: 83.jpg\n",
      "Moved: 133.jpg\n",
      "Moved: 144.jpg\n",
      "Moved: 88.jpg\n",
      "Moved: 143.jpg\n",
      "Moved: 39.jpg\n",
      "Moved: 140.jpg\n",
      "Moved: 151.jpg\n",
      "Moved: 64.jpg\n",
      "Moved: 118.jpg\n",
      "Moved: 128.jpg\n",
      "Moved: 15.jpg\n",
      "Moved: 168.jpg\n",
      "Moved: 94.jpg\n",
      "Moved: 157.jpg\n",
      "Moved: 0.jpg\n",
      "Moved: 40.jpg\n",
      "Moved: 97.jpg\n",
      "Moved: 106.jpg\n",
      "Moved: 48.jpg\n",
      "Moved: 61.jpg\n",
      "Moved: 112.jpg\n",
      "Moved: 153.jpg\n",
      "Moved: 87.jpg\n",
      "Moved: 75.jpg\n",
      "Moved: 67.jpg\n",
      "Moved: 119.jpg\n",
      "Moved: 114.jpg\n",
      "Moved: 8.jpg\n",
      "Moved: 108.jpg\n",
      "Moved: 14.jpg\n",
      "Moved: 131.jpg\n",
      "Moved: 185.jpg\n",
      "Moved: 53.jpg\n",
      "Moved: 186.jpg\n",
      "Moved: 134.jpg\n",
      "Moved: 110.jpg\n",
      "Moved: 166.jpg\n",
      "Moved: 24.jpg\n",
      "Moved: 176.jpg\n",
      "Moved: 34.jpg\n",
      "Moved: 147.jpg\n",
      "Moved: 135.jpg\n",
      "Moved: 56.jpg\n",
      "Moved: 116.jpg\n",
      "Moved: 145.jpg\n",
      "Moved: 169.jpg\n",
      "Moved: 49.jpg\n",
      "Moved: 5.jpg\n",
      "Moved: 127.jpg\n",
      "Moved: 109.jpg\n",
      "Moved: 7.jpg\n",
      "Moved: 85.jpg\n",
      "Moved: 183.jpg\n",
      "Moved: 149.jpg\n",
      "Moved: 68.jpg\n",
      "Moved: 77.jpg\n",
      "Moved: 19.jpg\n",
      "Moved: 16.jpg\n",
      "Moved: 72.jpg\n",
      "Moved: 148.jpg\n",
      "Moved: 189.jpg\n",
      "Moved: 150.jpg\n",
      "Moved: 78.jpg\n",
      "Moved: 51.jpg\n",
      "Moved: 80.jpg\n",
      "Moved: 29.jpg\n",
      "Moved: 187.jpg\n",
      "Moved: 6.jpg\n",
      "Moved: 101.jpg\n",
      "Moved: 62.jpg\n",
      "Moved: 52.jpg\n",
      "Moved: 46.jpg\n",
      "Moved: 136.jpg\n",
      "Moved: 28.jpg\n",
      "Moved: 159.jpg\n",
      "Moved: 9.jpg\n",
      "Moved: 74.jpg\n",
      "Moved: 54.jpg\n",
      "Moved: 181.jpg\n",
      "Moved: 193.jpg\n",
      "Moved: 137.jpg\n",
      "Moved: 113.jpg\n",
      "Moved: 160.jpg\n",
      "Moved: 122.jpg\n",
      "Moved: 141.jpg\n",
      "Moved: 152.jpg\n",
      "Moved: 59.jpg\n",
      "Moved: 33.jpg\n",
      "Moved: 11.jpg\n",
      "Moved: 82.jpg\n",
      "Moved: 163.jpg\n",
      "Moved: 99.jpg\n",
      "Moved: 174.jpg\n",
      "Moved: 69.jpg\n",
      "Moved: 58.jpg\n",
      "Moved: 177.jpg\n",
      "Moved: 70.jpg\n",
      "Moved: 4.jpg\n",
      "Moved: 25.jpg\n",
      "Moved: 98.jpg\n",
      "Moved: 44.jpg\n",
      "Moved: 73.jpg\n",
      "Moved: 63.jpg\n",
      "Moved: 125.jpg\n",
      "Moved: 17.jpg\n",
      "Moved: 158.jpg\n",
      "Moved: 194.jpg\n",
      "Moved: 171.jpg\n",
      "Moved: 104.jpg\n",
      "Moved: 164.jpg\n",
      "Moved: 130.jpg\n",
      "Moved: 13.jpg\n",
      "Moved: 103.jpg\n",
      "Moved: 156.jpg\n",
      "Moved: 175.jpg\n",
      "Moved: 121.jpg\n",
      "Moved: 124.jpg\n",
      "Moved: 41.jpg\n",
      "Moved: 95.jpg\n",
      "Moved: 42.jpg\n",
      "Moved: 47.jpg\n",
      "Moved: 107.jpg\n",
      "Moved: 76.jpg\n",
      "Moved: 102.jpg\n",
      "Moved: 139.jpg\n",
      "Moved: 165.jpg\n",
      "Moved: 37.jpg\n",
      "Moved: 120.jpg\n",
      "Moved: 167.jpg\n",
      "Moved: 105.jpg\n",
      "Moved: 180.jpg\n",
      "Moved: 20.jpg\n",
      "Moved: 90.jpg\n",
      "Moved: 71.jpg\n",
      "Moved: 184.jpg\n",
      "Moved: 31.jpg\n",
      "Moved: 3.jpg\n",
      "Moved: 10.jpg\n",
      "Moved: 86.jpg\n",
      "Moved: 182.jpg\n",
      "Moved: 195.jpg\n",
      "Moved: 36.jpg\n",
      "Moved: 142.jpg\n",
      "Moved: 154.jpg\n",
      "Moved: 162.jpg\n",
      "Moved: 22.jpg\n",
      "Moved: 173.jpg\n",
      "Moved: 26.jpg\n",
      "Moved: 91.jpg\n",
      "Moved: 30.jpg\n",
      "Moved: 161.jpg\n",
      "Moved: 126.jpg\n",
      "Moved: 43.jpg\n",
      "Moved: 23.jpg\n",
      "Moved: 138.jpg\n",
      "Moved: 100.jpg\n",
      "Moved: 111.jpg\n",
      "Moved: 50.jpg\n",
      "Moved: 60.jpg\n",
      "Moved: 190.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def move_random_images(source_dir, destination_dir, num_images_to_move):\n",
    "    # Get a list of all files in the source directory\n",
    "    all_images = [f for f in os.listdir(source_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "    # Randomly select the specified number of images\n",
    "    selected_images = random.sample(all_images, min(num_images_to_move, len(all_images)))\n",
    "\n",
    "    # Move the selected images to the destination directory\n",
    "    for image in selected_images:\n",
    "        source_path = os.path.join(source_dir, image)\n",
    "        destination_path = os.path.join(destination_dir, image)\n",
    "        shutil.move(source_path, destination_path)\n",
    "        print(f'Moved: {image}')\n",
    "\n",
    "# Replace these paths with your actual source and destination directories\n",
    "source_directory = 'C:\\\\Users\\\\User\\\\FYP\\\\DownloadedImages\\\\LAION-400M\\\\doctorImagesFiltered'\n",
    "destination_directory = 'C:\\\\Users\\\\User\\\\FYP\\DownloadedImages\\\\LAION-400M\\\\doctorImageSubsetsForProcessing\\\\tmp'\n",
    "\n",
    "# Specify the number of images you want to move\n",
    "num_images_to_move = 385\n",
    "\n",
    "# Call the function to move random images\n",
    "move_random_images(source_directory, destination_directory, num_images_to_move)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Snippet 2\n",
    "Description: Code used to determine which images have only a specified number of people in them and copy those images to another directoy.\n",
    "\n",
    "Note: This was used to select the 97 images used for the Google Form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 persons, 267.1ms\n",
      "Speed: 6.0ms preprocess, 267.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 246.1ms\n",
      "Speed: 5.0ms preprocess, 246.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 252.1ms\n",
      "Speed: 5.0ms preprocess, 252.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 237.1ms\n",
      "Speed: 5.0ms preprocess, 237.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 257.1ms\n",
      "Speed: 6.0ms preprocess, 257.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 253.1ms\n",
      "Speed: 6.0ms preprocess, 253.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 263.1ms\n",
      "Speed: 6.0ms preprocess, 263.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 256.1ms\n",
      "Speed: 5.0ms preprocess, 256.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 249.1ms\n",
      "Speed: 6.0ms preprocess, 249.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 295.1ms\n",
      "Speed: 6.0ms preprocess, 295.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dlib\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# source_dir = 'C:\\\\Users\\\\User\\\\FYP\\\\MidJourney\\\\Nurse'   \n",
    "# source_dir = \"C:\\\\Users\\\\User\\\\FYP\\\\StableDiffusion\\\\StableDiffusion\\\\stable-diffusion-webui\\\\outputs\\\\txt2img-images\\\\Doctor&Nurse(DPM++ 2M Karras)\"\n",
    "source_dir = \"C:\\\\Users\\\\User\\\\FYP\\\\StableDiffusion\\\\StableDiffusion\\\\stable-diffusion-webui\\\\outputs\\\\txt2img-images\\\\2024-03-16\"\n",
    "\n",
    "all_images = [f for f in os.listdir(source_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "# Loading the YOLO model. This is downloaded automatically when run for the first time.\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "person_images = []\n",
    "confidence_threshold = 0.5\n",
    "number_of_people = 2\n",
    "# num_images = 97\n",
    "\n",
    "for image in all_images:\n",
    "    input_image_path = os.path.join(source_dir, image)\n",
    "    image = dlib.load_rgb_image(input_image_path)\n",
    "    # Making predictions\n",
    "    predictions = model.predict(image, classes=0)\n",
    "\n",
    "    scores = predictions[0].boxes.conf\n",
    "    filtered_indices = torch.where(scores > confidence_threshold)[0]\n",
    "\n",
    "    if len(filtered_indices) == number_of_people:\n",
    "        person_images.append(image) \n",
    "\n",
    "# Randomly select the specified number of images\n",
    "selected_images = person_images#random.sample(person_images, min(num_images, len(person_images)))\n",
    "\n",
    "for imgIndex, image in enumerate(selected_images):\n",
    "    # save_path = 'C:\\\\Users\\\\User\\\\FYP\\\\MidJourney\\\\Nurse1Person'\n",
    "    save_path = \"C:\\\\Users\\\\User\\\\FYP\\\\StableDiffusion\\\\StableDiffusion\\\\stable-diffusion-webui\\\\outputs\\\\txt2img-images\\\\result\"   \n",
    "    save_path = os.path.join(save_path,str(imgIndex)+\".jpg\")\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dlib\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import pandas as pd\n",
    "\n",
    "source_dir = 'C:\\\\Users\\\\User\\\\FYP\\\\Dall-E\\\\GeneratedImages\\\\NurseFinalisedFiltering'  \n",
    "data_csv_path = 'C:\\\\Users\\\\User\\\\FYP\\\\Dall-E\\\\GeneratedImages\\\\NurseFinalisedFiltering\\\\data.csv'   \n",
    "\n",
    "# Load data.csv\n",
    "data_csv = pd.read_csv(data_csv_path)\n",
    "# Convert DataFrame to dictionary\n",
    "data_csv = data_csv.to_dict(orient='records')\n",
    "\n",
    "all_images = [f for f in os.listdir(source_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "saved_images_data = []\n",
    "\n",
    "imgIndex = 0\n",
    "for image in all_images:\n",
    "    image_name = image\n",
    "    input_image_path = os.path.join(source_dir, image)\n",
    "    image = dlib.load_rgb_image(input_image_path)\n",
    "\n",
    "    # Using the mtcnn detector to detect faces in the image\n",
    "    objs = DeepFace.analyze(image, enforce_detection=False,  detector_backend=\"mtcnn\", silent=True)\n",
    "\n",
    "    if len(objs) == 2:\n",
    "        save_path = 'C:\\\\Users\\\\User\\\\FYP\\\\Dall-E\\\\GeneratedImages\\\\NurseFinalisedFilteringOrdered'\n",
    "        save_path = os.path.join(save_path,str(imgIndex)+\".jpg\")\n",
    "        cv2.imwrite(save_path, cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) \n",
    "\n",
    "        image_index = next((index for index, item in enumerate(data_csv) if item.get('image_no') == int(image_name.split('.')[0])), None)\n",
    "        saved_images_data.append(data_csv[image_index])\n",
    "        imgIndex += 1\n",
    "\n",
    "saved_images_data_csv = pd.DataFrame(saved_images_data)\n",
    "saved_images_data_csv.to_csv('C:\\\\Users\\\\User\\\\FYP\\\\Dall-E\\\\GeneratedImages\\\\NurseFinalisedFilteringOrdered\\\\data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Snippet 3\n",
    "\n",
    "Description: Code used to load and process the data retrieved from the Google Forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Gender_User</th>\n",
       "      <th>Race_User</th>\n",
       "      <th>Age_User</th>\n",
       "      <th>Gender_0</th>\n",
       "      <th>Race_0</th>\n",
       "      <th>Age_0</th>\n",
       "      <th>Gender_1</th>\n",
       "      <th>Race_1</th>\n",
       "      <th>Age_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Age_93</th>\n",
       "      <th>Gender_94</th>\n",
       "      <th>Race_94</th>\n",
       "      <th>Age_94</th>\n",
       "      <th>Gender_95</th>\n",
       "      <th>Race_95</th>\n",
       "      <th>Age_95</th>\n",
       "      <th>Gender_96</th>\n",
       "      <th>Race_96</th>\n",
       "      <th>Age_96</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024/03/09 2:44:44 pm CET</td>\n",
       "      <td>Male</td>\n",
       "      <td>10</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>Male</td>\n",
       "      <td>Latino Hispanic</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>54.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024/03/09 2:47:48 pm CET</td>\n",
       "      <td>Female</td>\n",
       "      <td>78</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>78.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>90.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024/03/09 2:44:44 pm CET</td>\n",
       "      <td>Male</td>\n",
       "      <td>10</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024/03/09 2:47:48 pm CET</td>\n",
       "      <td>Female</td>\n",
       "      <td>78</td>\n",
       "      <td>Austria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024/03/09 2:44:44 pm CET</td>\n",
       "      <td>Male</td>\n",
       "      <td>10</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024/03/09 2:47:48 pm CET</td>\n",
       "      <td>Female</td>\n",
       "      <td>78</td>\n",
       "      <td>Austria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024/03/09 2:44:44 pm CET</td>\n",
       "      <td>Male</td>\n",
       "      <td>10</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Latino Hispanic</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024/03/09 2:47:48 pm CET</td>\n",
       "      <td>Female</td>\n",
       "      <td>78</td>\n",
       "      <td>Austria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Latino Hispanic</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Latino Hispanic</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 295 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Timestamp Gender_User  Race_User Age_User Gender_0  \\\n",
       "0  2024/03/09 2:44:44 pm CET        Male         10  Armenia     Male   \n",
       "1  2024/03/09 2:47:48 pm CET      Female         78  Austria     Male   \n",
       "2  2024/03/09 2:44:44 pm CET        Male         10  Armenia      NaN   \n",
       "3  2024/03/09 2:47:48 pm CET      Female         78  Austria      NaN   \n",
       "4  2024/03/09 2:44:44 pm CET        Male         10  Armenia      NaN   \n",
       "5  2024/03/09 2:47:48 pm CET      Female         78  Austria      NaN   \n",
       "6  2024/03/09 2:44:44 pm CET        Male         10  Armenia      NaN   \n",
       "7  2024/03/09 2:47:48 pm CET      Female         78  Austria      NaN   \n",
       "\n",
       "            Race_0  Age_0 Gender_1 Race_1  Age_1  ... Age_93 Gender_94  \\\n",
       "0  Latino Hispanic   12.0   Female  Black   54.0  ...    NaN       NaN   \n",
       "1            Asian   78.0     Male  White   90.0  ...    NaN       NaN   \n",
       "2              NaN    NaN      NaN    NaN    NaN  ...    NaN       NaN   \n",
       "3              NaN    NaN      NaN    NaN    NaN  ...    NaN       NaN   \n",
       "4              NaN    NaN      NaN    NaN    NaN  ...    NaN       NaN   \n",
       "5              NaN    NaN      NaN    NaN    NaN  ...    NaN       NaN   \n",
       "6              NaN    NaN      NaN    NaN    NaN  ...   55.0    Female   \n",
       "7              NaN    NaN      NaN    NaN    NaN  ...   31.0      Male   \n",
       "\n",
       "           Race_94 Age_94 Gender_95  Race_95 Age_95 Gender_96  \\\n",
       "0              NaN    NaN       NaN      NaN    NaN       NaN   \n",
       "1              NaN    NaN       NaN      NaN    NaN       NaN   \n",
       "2              NaN    NaN       NaN      NaN    NaN       NaN   \n",
       "3              NaN    NaN       NaN      NaN    NaN       NaN   \n",
       "4              NaN    NaN       NaN      NaN    NaN       NaN   \n",
       "5              NaN    NaN       NaN      NaN    NaN       NaN   \n",
       "6  Latino Hispanic  100.0    Female    Asian   12.0    Female   \n",
       "7  Latino Hispanic   13.0    Female    White   31.0      Male   \n",
       "\n",
       "           Race_96 Age_96  \n",
       "0              NaN    NaN  \n",
       "1              NaN    NaN  \n",
       "2              NaN    NaN  \n",
       "3              NaN    NaN  \n",
       "4              NaN    NaN  \n",
       "5              NaN    NaN  \n",
       "6           Indian   42.0  \n",
       "7  Latino Hispanic   31.0  \n",
       "\n",
       "[8 rows x 295 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: The NAN values in the CSV file are present as those images were annotated by other users. \n",
    "# For instance image 0 was annoted by user 1 & 2 but image 25 was annoted by users 3 & 4 so \n",
    "# image 0 has NaN values for user 3 & 4 and image 25 has NaN values for user 1 & 2.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file.csv' with the actual path to your CSV file\n",
    "file_path = ['GoogleFormResponses\\\\Doctor\\\\Doctor Annotation - 1.csv',\n",
    "             'GoogleFormResponses\\\\Doctor\\\\Doctor Annotation - 2.csv',\n",
    "             'GoogleFormResponses\\\\Doctor\\\\Doctor Annotation - 3.csv',\n",
    "             'GoogleFormResponses\\\\Doctor\\\\Doctor Annotation - 4.csv']\n",
    "\n",
    "# Initialize an empty DataFrame to store the concatenated data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Loop through each CSV file and concatenate the DataFrames\n",
    "for index, csv_file in enumerate(file_path):\n",
    "    part_df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Changing the column names for easier access\n",
    "    # List of column names\n",
    "    column_names = part_df.keys()\n",
    "\n",
    "    column_mapping = {}\n",
    "    column_mapping[column_names[1]] = \"Gender_User\"\n",
    "    column_mapping[column_names[2]] = \"Race_User\"\n",
    "    column_mapping[column_names[3]] = \"Age_User\"\n",
    "\n",
    "    image_number = index*25\n",
    "    # Loop through the column names, starting from the 5th column\n",
    "    for index in range(4,len(column_names),3):\n",
    "        column_mapping[column_names[index]] = \"Gender_\"+str(image_number)\n",
    "        column_mapping[column_names[index+1]] = \"Race_\"+str(image_number)\n",
    "        column_mapping[column_names[index+2]] = \"Age_\"+str(image_number)\n",
    "        image_number += 1\n",
    "    # Rename the columns using the rename method\n",
    "    part_df = part_df.rename(columns=column_mapping)\n",
    "    df = pd.concat([df, part_df], ignore_index=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'age': [12.0, 78.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Latino Hispanic', 'Asian']},\n",
       " 1: {'age': [54.0, 90.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Black', 'White']},\n",
       " 2: {'age': [32.0, 99.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Asian', 'Black']},\n",
       " 3: {'age': [66.0, 98.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Indian', 'White']},\n",
       " 4: {'age': [31.0, 78.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['White', 'Black']},\n",
       " 5: {'age': [12.0, 90.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Black', 'White']},\n",
       " 6: {'age': [22.0, 89.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['White', 'Latino Hispanic']},\n",
       " 7: {'age': [22.0, 89.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Indian', 'Middle Eastern']},\n",
       " 8: {'age': [44.0, 23.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Indian', 'Black']},\n",
       " 9: {'age': [44.0, 42.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['White', 'Black']},\n",
       " 10: {'age': [44.0, 42.0],\n",
       "  'gender': ['Female', 'Female'],\n",
       "  'race': ['Asian', 'Indian']},\n",
       " 11: {'age': [12.0, 55.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Black', 'Latino Hispanic']},\n",
       " 12: {'age': [12.0, 13.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Latino Hispanic', 'Latino Hispanic']},\n",
       " 13: {'age': [31.0, 13.0],\n",
       "  'gender': ['Male', 'Female'],\n",
       "  'race': ['Latino Hispanic', 'Latino Hispanic']},\n",
       " 14: {'age': [12.0, 31.0],\n",
       "  'gender': ['Female', 'Female'],\n",
       "  'race': ['Black', 'Black']},\n",
       " 15: {'age': [43.0, 31.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Middle Eastern', 'Indian']},\n",
       " 16: {'age': [12.0, 13.0],\n",
       "  'gender': ['Male', 'Female'],\n",
       "  'race': ['Black', 'Black']},\n",
       " 17: {'age': [12.0, 13.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['White', 'Black']},\n",
       " 18: {'age': [55.0, 31.0],\n",
       "  'gender': ['Female', 'Female'],\n",
       "  'race': ['Black', 'Black']},\n",
       " 19: {'age': [100.0, 13.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Latino Hispanic', 'Latino Hispanic']},\n",
       " 20: {'age': [12.0, 31.0],\n",
       "  'gender': ['Female', 'Female'],\n",
       "  'race': ['Asian', 'White']},\n",
       " 21: {'age': [42.0, 31.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Indian', 'Latino Hispanic']},\n",
       " 22: {'age': [41.0, 31.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Black', 'Indian']},\n",
       " 23: {'age': [12.0, 31.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Middle Eastern', 'Black']},\n",
       " 24: {'age': [22.0, 31.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['White', 'Black']},\n",
       " 25: {'age': [12.0, 78.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Latino Hispanic', 'Asian']},\n",
       " 26: {'age': [54.0, 90.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Black', 'White']},\n",
       " 27: {'age': [32.0, 99.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Asian', 'Black']},\n",
       " 28: {'age': [66.0, 98.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Indian', 'White']},\n",
       " 29: {'age': [31.0, 78.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['White', 'Black']},\n",
       " 30: {'age': [12.0, 90.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Black', 'White']},\n",
       " 31: {'age': [22.0, 89.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['White', 'Latino Hispanic']},\n",
       " 32: {'age': [22.0, 89.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Indian', 'Middle Eastern']},\n",
       " 33: {'age': [44.0, 23.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Indian', 'Black']},\n",
       " 34: {'age': [44.0, 42.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['White', 'Black']},\n",
       " 35: {'age': [44.0, 42.0],\n",
       "  'gender': ['Female', 'Female'],\n",
       "  'race': ['Asian', 'Indian']},\n",
       " 36: {'age': [12.0, 55.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Black', 'Latino Hispanic']},\n",
       " 37: {'age': [12.0, 13.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Latino Hispanic', 'Latino Hispanic']},\n",
       " 38: {'age': [31.0, 13.0],\n",
       "  'gender': ['Male', 'Female'],\n",
       "  'race': ['Latino Hispanic', 'Latino Hispanic']},\n",
       " 39: {'age': [12.0, 31.0],\n",
       "  'gender': ['Female', 'Female'],\n",
       "  'race': ['Black', 'Black']},\n",
       " 40: {'age': [43.0, 31.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Middle Eastern', 'Indian']},\n",
       " 41: {'age': [12.0, 13.0],\n",
       "  'gender': ['Male', 'Female'],\n",
       "  'race': ['Black', 'Black']},\n",
       " 42: {'age': [12.0, 13.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['White', 'Black']},\n",
       " 43: {'age': [55.0, 31.0],\n",
       "  'gender': ['Female', 'Female'],\n",
       "  'race': ['Black', 'Black']},\n",
       " 44: {'age': [100.0, 13.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Latino Hispanic', 'Latino Hispanic']},\n",
       " 45: {'age': [12.0, 31.0],\n",
       "  'gender': ['Female', 'Female'],\n",
       "  'race': ['Asian', 'White']},\n",
       " 46: {'age': [42.0, 31.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Indian', 'Latino Hispanic']},\n",
       " 47: {'age': [41.0, 31.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Black', 'Indian']},\n",
       " 48: {'age': [12.0, 31.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Middle Eastern', 'Black']},\n",
       " 49: {'age': [22.0, 31.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['White', 'Black']},\n",
       " 50: {'age': [12.0, 78.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Latino Hispanic', 'Asian']},\n",
       " 51: {'age': [54.0, 90.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Black', 'White']},\n",
       " 52: {'age': [32.0, 99.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Asian', 'Black']},\n",
       " 53: {'age': [66.0, 98.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Indian', 'White']},\n",
       " 54: {'age': [31.0, 78.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['White', 'Black']},\n",
       " 55: {'age': [12.0, 90.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Black', 'White']},\n",
       " 56: {'age': [22.0, 89.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['White', 'Latino Hispanic']},\n",
       " 57: {'age': [22.0, 89.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Indian', 'Middle Eastern']},\n",
       " 58: {'age': [44.0, 23.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Indian', 'Black']},\n",
       " 59: {'age': [44.0, 42.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['White', 'Black']},\n",
       " 60: {'age': [44.0, 42.0],\n",
       "  'gender': ['Female', 'Female'],\n",
       "  'race': ['Asian', 'Indian']},\n",
       " 61: {'age': [12.0, 55.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Black', 'Latino Hispanic']},\n",
       " 62: {'age': [12.0, 13.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Latino Hispanic', 'Latino Hispanic']},\n",
       " 63: {'age': [31.0, 13.0],\n",
       "  'gender': ['Male', 'Female'],\n",
       "  'race': ['Latino Hispanic', 'Latino Hispanic']},\n",
       " 64: {'age': [12.0, 31.0],\n",
       "  'gender': ['Female', 'Female'],\n",
       "  'race': ['Black', 'Black']},\n",
       " 65: {'age': [43.0, 31.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Middle Eastern', 'Indian']},\n",
       " 66: {'age': [12.0, 13.0],\n",
       "  'gender': ['Male', 'Female'],\n",
       "  'race': ['Black', 'Black']},\n",
       " 67: {'age': [12.0, 13.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['White', 'Black']},\n",
       " 68: {'age': [55.0, 31.0],\n",
       "  'gender': ['Female', 'Female'],\n",
       "  'race': ['Black', 'Black']},\n",
       " 69: {'age': [100.0, 13.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Latino Hispanic', 'Latino Hispanic']},\n",
       " 70: {'age': [12.0, 31.0],\n",
       "  'gender': ['Female', 'Female'],\n",
       "  'race': ['Asian', 'White']},\n",
       " 71: {'age': [42.0, 31.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Indian', 'Latino Hispanic']},\n",
       " 72: {'age': [41.0, 31.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Black', 'Indian']},\n",
       " 73: {'age': [12.0, 31.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Middle Eastern', 'Black']},\n",
       " 74: {'age': [22.0, 31.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['White', 'Black']},\n",
       " 75: {'age': [12.0, 78.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Latino Hispanic', 'Asian']},\n",
       " 76: {'age': [54.0, 90.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Black', 'White']},\n",
       " 77: {'age': [32.0, 99.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Asian', 'Black']},\n",
       " 78: {'age': [66.0, 98.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Indian', 'White']},\n",
       " 79: {'age': [31.0, 78.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['White', 'Black']},\n",
       " 80: {'age': [12.0, 90.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Black', 'White']},\n",
       " 81: {'age': [22.0, 89.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['White', 'Latino Hispanic']},\n",
       " 82: {'age': [22.0, 89.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Indian', 'Middle Eastern']},\n",
       " 83: {'age': [44.0, 23.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Indian', 'Black']},\n",
       " 84: {'age': [44.0, 42.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['White', 'Black']},\n",
       " 85: {'age': [44.0, 42.0],\n",
       "  'gender': ['Female', 'Female'],\n",
       "  'race': ['Asian', 'Indian']},\n",
       " 86: {'age': [12.0, 55.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Black', 'Latino Hispanic']},\n",
       " 87: {'age': [12.0, 13.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Latino Hispanic', 'Latino Hispanic']},\n",
       " 88: {'age': [31.0, 13.0],\n",
       "  'gender': ['Male', 'Female'],\n",
       "  'race': ['Latino Hispanic', 'Latino Hispanic']},\n",
       " 89: {'age': [12.0, 31.0],\n",
       "  'gender': ['Female', 'Female'],\n",
       "  'race': ['Black', 'Black']},\n",
       " 90: {'age': [43.0, 31.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['Middle Eastern', 'Indian']},\n",
       " 91: {'age': [12.0, 13.0],\n",
       "  'gender': ['Male', 'Female'],\n",
       "  'race': ['Black', 'Black']},\n",
       " 92: {'age': [12.0, 13.0],\n",
       "  'gender': ['Male', 'Male'],\n",
       "  'race': ['White', 'Black']},\n",
       " 93: {'age': [55.0, 31.0],\n",
       "  'gender': ['Female', 'Female'],\n",
       "  'race': ['Black', 'Black']},\n",
       " 94: {'age': [100.0, 13.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Latino Hispanic', 'Latino Hispanic']},\n",
       " 95: {'age': [12.0, 31.0],\n",
       "  'gender': ['Female', 'Female'],\n",
       "  'race': ['Asian', 'White']},\n",
       " 96: {'age': [42.0, 31.0],\n",
       "  'gender': ['Female', 'Male'],\n",
       "  'race': ['Indian', 'Latino Hispanic']}}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving the image data, the Nan values are removed for the sake of simplicity\n",
    "image_set_data = {}\n",
    "\n",
    "# The number of images is calculated by subtracting 4 (Timestamp/Gender_User/Race_User/Age_User) \n",
    "# from the total number of columns and then dividing by 3 (Number of labels for each image)\n",
    "number_of_images = (len(df.keys())-4)/3\n",
    "for image_index in range(0,number_of_images):\n",
    "    image_data = {}\n",
    "    image_data[\"age\"] = list(df[\"Age_\"+str(image_index)].dropna())\n",
    "    image_data[\"gender\"] = list(df[\"Gender_\"+str(image_index)].dropna())\n",
    "    image_data[\"race\"] = list(df[\"Race_\"+str(image_index)].dropna())\n",
    "    image_set_data[image_index] = image_data\n",
    "\n",
    "image_set_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
